{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import json\n",
    "import openai\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import configparser\n",
    "import time\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This program uses openAI via API.  It requires an API key to be saved in the config file, and the GPT version can be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSave your openAI key in config.ini in the below format : \\n\\n[DEFAULT]\\nopenai_api_key = .....\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llmModel = \"gpt-3.5-turbo\"\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "openai.api_key =config['DEFAULT']['openai_api_key']\n",
    "\n",
    "'''\n",
    "Save your openAI key in config.ini in the below format : \n",
    "\n",
    "[DEFAULT]\n",
    "openai_api_key = .....\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a dataset for the analysis.  Here we'll use a classic of machine learning libraries.  This dataset doesn't have the description loaded on Huggingface in the metadata, so I load a brief description into the local variable dsInfo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"scikit-learn/auto-mpg\",split='train')\n",
    "df=pd.DataFrame(dataset)\n",
    "\n",
    "\n",
    "dsInfo = \"car mpg statistics\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defintions of the analysis functions that we want to enable the application with.  Here they are coded in python, and then described in JSON for the LLM.  A future improvement would be to convert these to pydantic functions, which is a more concise method of creating the JSON schema directly from the function definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterDataset(target,targetOperator,targetValue):\n",
    "    print(f\"At filter database, got {target} and {targetOperator} and {targetValue}!!!\")\n",
    "\n",
    "\n",
    "def aggGroupBy(target,groupList=None,functionList=['mean'],filterConditions=None,precision=2):\n",
    "    if ',' in functionList:\n",
    "        functionList=functionList.split(',')\n",
    "        functionList = list([x.strip() for x in functionList])\n",
    "    if type(groupList)!=list:\n",
    "        groupList = [groupList]\n",
    "    if type(functionList)!=list:\n",
    "        functionList=[functionList]\n",
    "    if groupList is None:\n",
    "        report=df.agg({target:functionList}).round(precision)\n",
    "    if filterConditions is None:\n",
    "        report=df.groupby(groupList).agg({target:functionList}).round(precision)\n",
    "    else:\n",
    "        print(filterConditions)\n",
    "        report=df.groupby(groupList).agg({target:functionList}).round(precision)\n",
    "    return report.head(20).to_csv()\n",
    "\n",
    "\n",
    "def minOrMaxCase(target,aggFunction):\n",
    "    if aggFunction == 'max':\n",
    "        rowOrder = False\n",
    "    else:\n",
    "        rowOrder=True\n",
    "    if target=='acceleration' and rowOrder==False:\n",
    "        rowOrder = True\n",
    "    elif target=='acceleration':\n",
    "        rowOrder = False\n",
    "    report=df.sort_values(target,ascending=rowOrder).head(1)\n",
    "    report = json.dumps(report.to_dict(orient='records')[0])\n",
    "    return report\n",
    "\n",
    "\n",
    "def multipleRegression(indVar=['displacement'],depVar='mpg'):\n",
    "    if ',' in indVar:\n",
    "        indVar = [x.strip() for x in indVar.split(',')]\n",
    "    if type(indVar) !=list:\n",
    "        indVar = [indVar]\n",
    "    y=df[depVar].values.reshape(-1,1)\n",
    "    X=df[indVar]#.values.reshape(-1,1)\n",
    "    reg = LinearRegression().fit(X, y)\n",
    "    score = round(reg.score(X, y),3)\n",
    "    coef = reg.coef_\n",
    "    intercept = round(reg.intercept_[0],3)\n",
    "    formula = depVar+\" = \"+str(intercept)+' '\n",
    "    for z in zip(coef[0],indVar):\n",
    "        if z[0]>0:\n",
    "            formula=formula+\"+\"\n",
    "        formula = formula+str(z[0].round(3))+\"*\"+z[1]+' '\n",
    "    report = f\"\"\"The independent variable(s) explain {score} of the variance in {depVar}.The R2 or R-squared value is {depVar}. The regression formula is {formula}\"\"\"\n",
    "    return report\n",
    "\n",
    "\n",
    "# JSON Schema for above functions\n",
    "\n",
    "\n",
    "functions = [\n",
    "    {\n",
    "        \"name\": \"aggGroupBy\",\n",
    "        \"description\": '''Makes an aggregated report from the dataset.\n",
    "          Lets the user specify a variable to be analyzed, one or more grouping variables, \n",
    "          and one or more aggregation functions from (min,max,mean,std).  You can also use pandas to filter the dataframe\n",
    "          by specifying filterConditions.''',\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"groupList\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"List of variable names to group by\",\n",
    "                },\n",
    "                \"target\": {\"type\": \"string\",\"description\":\"the variable name to be analyzed and aggregated\"},\n",
    "            \n",
    "            \"functionList\": {\"type\": \"string\",\"description\":\"list of pandas aggregation functions, chosen from min, max, mean, std\"},\n",
    "            \n",
    "            \"filterConditions\": {\"type\": \"string\",\"description\":\"pandas where conditions to filter the dataset with\"},\n",
    "            },},\n",
    "            \"required\": [\"target\", \"functionList\"],\n",
    "        },\n",
    "    {\n",
    "        \"name\": \"minOrMaxCase\",\n",
    "        \"description\": '''Finds one case from the dataframe with a max or min value on the specified target variable.''',\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"aggFunction\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Specifies if the user wants the argmax or argmin.  return either 'min' or 'max'\",\n",
    "                },\n",
    "                \"target\": {\"type\": \"string\",\"description\":\"the variable name to be analyzed and aggregated\"},\n",
    "            },\n",
    "            \"aggregations\": {\"type\": \"string\",\"description\":\"list of pandas aggregation functions, including min, max\"},\n",
    "            \n",
    "            \"filterConditions\": {\"type\": \"string\",\"description\":\"pandas where conditions to filter the dataset with\"},\n",
    "            },\n",
    "            \"required\": [\"target\"],\n",
    "        },\n",
    "        {\n",
    "        \"name\": \"multipleRegression\",\n",
    "        \"description\": '''runs a linear multiple regression to determine the linear relationship between the dependent \\n\n",
    "                            target variable and one or more independent explanatory variables. \\n\n",
    "                            Returns the R2 (R-squared) value and the linear formula.''',\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"indVar\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"one or more independent variables to explain the dependent variable via linear multiple regression\",\n",
    "                },\n",
    "                \"depVar\": {\"type\": \"string\",\"description\":\"one variable to be explained by regression by the independent variable(s)\"},\n",
    "            },\n",
    "            },\n",
    "            \"required\": [\"indVar\",\"depVar\"],\n",
    "        },\n",
    "                {\n",
    "        \"name\": \"filterDataset\",\n",
    "        \"description\": '''filters the dataset using conditional logic of GT, GTE, LT, LTE against one variable with one parameter value''',\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"target\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"one valid variable name to filter the dataset by\",\n",
    "                },\n",
    "                \"targetOperator\": {\"type\": \"string\",\"description\":\"valid values are GT, GTE, LT or LTE, and represent the condition to use against the target\"},\n",
    "            },\n",
    "            \"targetValues\": {\"type\": \"string\",\"description\":\"value to use in the conditional with the target and targetOperator\"},\n",
    "            },\n",
    "            },\n",
    "            \"required\": [\"indVar\",\"depVar\"],\n",
    "        }\n",
    "    \n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chat stream needs to be initialized with the LLM.  Here the initial system instructions are defined, using some metadata from the dataframe we are loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "varnames = ', '.join(list(df.columns))\n",
    "\n",
    "\n",
    "initialMessages = [\n",
    "        {  \n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"You are a helpful assistant that analyzes a dataset about \"\"\"+dsInfo+\"\"\"for the user. \n",
    "        If the user's question is not directly related to the dataset, politely reject it.\n",
    "        You can use function calls to get data, or respond in sentences when proovided the data.\n",
    "        If you don't know how to apply to a function, apoligize and say you don't know how to do that.\n",
    "          You can answer questions about these variable names :\"\"\"+varnames+\"\"\"\n",
    "        All parameter values must be one of the specified variable names.\n",
    "        Do not make up information, and keep your responses concise.\n",
    "        If you identify the need to reply with a function call, but some required parameter are missing,\n",
    "          reply with a follow up question asking for missing parameters.\n",
    "          You can also answer questions about the dataset metadata, and explain statistical analyses.\"\"\"\n",
    "    },\n",
    "]\n",
    "\n",
    "messages = initialMessages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining functions that the chatbot will use to control flow and direct logic to and from the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clearItOut():\n",
    "    ''' Clear the message history and reinitialize with the system instructions.'''\n",
    "    global messages\n",
    "    messages = initialMessages\n",
    "\n",
    "\n",
    "def classifyReponse(response):\n",
    "    ''' Determine if the response from the LLM is providing content, or requesting a function to be run.'''\n",
    "    if response['choices'][0]['message']['content'] is not None:\n",
    "        return 'content'\n",
    "    elif 'function_call' in response['choices'][0]['message']:\n",
    "        return 'function_call'\n",
    "    \n",
    "\n",
    "def prepResponse(response):\n",
    "    ''' Either extracts content for the bot message, or runs a function and provides that output as the bot message'''\n",
    "    cr = classifyReponse(response)\n",
    "    if cr=='function_call':\n",
    "        f = response['choices'][0]['message']['function_call']['name']\n",
    "        kwargs = json.loads(response['choices'][0]['message']['function_call']['arguments'])\n",
    "        function_response = globals()[f](**kwargs)\n",
    "        #return formatted message and logic to call llm again\n",
    "        if type(function_response) is list:\n",
    "            function_response = ','.join(function_response)\n",
    "        message = {\n",
    "            \"role\": \"function\",\n",
    "            \"name\": f,\n",
    "            \"content\": function_response,\n",
    "        }\n",
    "        action = 'llm'\n",
    "        bot_message = None\n",
    "    elif cr=='content':\n",
    "        newContent = response['choices'][0]['message']['content']\n",
    "        #return formatted message and logic to send to ui\n",
    "        message = {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": newContent,\n",
    "        }\n",
    "        action = 'ui'\n",
    "        bot_message = newContent\n",
    "    return bot_message, message,action\n",
    "\n",
    "def callLLM(messages,functions):\n",
    "    ''' Helper function to make the call to the LLM '''\n",
    "    raw_message = openai.ChatCompletion.create(\n",
    "                model=llmModel,\n",
    "                messages=messages,\n",
    "                functions = functions, temperature = 0.\n",
    "            )\n",
    "    return raw_message\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jon/.local/lib/python3.10/site-packages/gradio/queueing.py\", line 388, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"/home/jon/.local/lib/python3.10/site-packages/gradio/route_utils.py\", line 219, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/jon/.local/lib/python3.10/site-packages/gradio/blocks.py\", line 1437, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/home/jon/.local/lib/python3.10/site-packages/gradio/blocks.py\", line 1123, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "  File \"/home/jon/.local/lib/python3.10/site-packages/gradio/utils.py\", line 512, in async_iteration\n",
      "    return await iterator.__anext__()\n",
      "  File \"/home/jon/.local/lib/python3.10/site-packages/gradio/utils.py\", line 505, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "  File \"/home/jon/.local/lib/python3.10/site-packages/anyio/to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/home/jon/.local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/home/jon/.local/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/home/jon/.local/lib/python3.10/site-packages/gradio/utils.py\", line 488, in run_sync_iterator_async\n",
      "    return next(iterator)\n",
      "  File \"/home/jon/.local/lib/python3.10/site-packages/gradio/utils.py\", line 638, in gen_wrapper\n",
      "    yield from f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_87222/1036254585.py\", line 25, in bot\n",
      "    bot_message, history_message,action = prepResponse(raw_message)\n",
      "  File \"/tmp/ipykernel_87222/1279916437.py\", line 21, in prepResponse\n",
      "    function_response = globals()[f](**kwargs)\n",
      "TypeError: filterDataset() got an unexpected keyword argument 'targetValue'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Keep track of the time the UI is started, so the history can be occassionally cleared out.\n",
    "startUpTime = dt.datetime.now()\n",
    "\n",
    "\n",
    "# Main chatbot loop\n",
    "\n",
    "with gr.Blocks(title=\"Conversational BI Demo\",theme='YenLai/Superhuman') as demo:\n",
    "    gr.Markdown(\"Use the textbox below to ask questions about the \"+dsInfo+\" dataset.\")\n",
    "    gr.Markdown(\"It contains information on these fields:\"+varnames)\n",
    "    chatbot = gr.Chatbot()\n",
    "    #msg = gr.Textbox(\"What is the car with the best mileage?\") #Pre load a question in the user input.\n",
    "    msg = gr.Textbox(\"Run a regression of mpg on cylinders and displacement\")\n",
    "    clear = gr.Button(\"Clear\")\n",
    "    gr.Markdown(\"To learn about this demo, go to my Github at https://github.com/jonathanmanly/runGearGuruconvBI\")\n",
    "\n",
    "    def user(user_message, history):\n",
    "        if ((dt.datetime.now()-startUpTime).seconds)/60>10:\n",
    "            clearItOut()\n",
    "        return \"\", history + [[user_message, None]]\n",
    "\n",
    "    def bot(history):\n",
    "        raw_message=''\n",
    "        messages.append({'role':'user',\"content\":history[-1][0]})\n",
    "        raw_message = callLLM(messages,functions)\n",
    "        bot_message, history_message,action = prepResponse(raw_message)\n",
    "        messages.append(history_message)\n",
    "        if action =='llm':\n",
    "            raw_message = callLLM(messages,functions)\n",
    "            #print(raw_message)\n",
    "            bot_message, history_message,action = prepResponse(raw_message)\n",
    "            messages.append(history_message)\n",
    "        \n",
    "        history[-1][1] = \"\"\n",
    "        for character in bot_message:\n",
    "            history[-1][1] += character\n",
    "            time.sleep(0.001)\n",
    "            yield history\n",
    "\n",
    "    msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(\n",
    "        bot, chatbot, chatbot\n",
    "    )\n",
    "    clear.click(clearItOut, None, chatbot, queue=False)\n",
    "    \n",
    "demo.queue()\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': \"You are a helpful assistant that analyzes a dataset about car mpg statisticsfor the user. \\n        If the user's question is not directly related to the dataset, politely reject it.\\n        You can use function calls to get data, or respond in sentences when proovided the data.\\n        If you don't know how to apply to a function, apoligize and say you don't know how to do that.\\n          You can answer questions about these variable names :mpg, cylinders, displacement, horsepower, weight, acceleration, model year, origin, car name\\n        All parameter values must be one of the specified variable names.\\n        Do not make up information, and keep your responses concise.\\n        If you identify the need to reply with a function call, but some required parameter are missing,\\n          reply with a follow up question asking for missing parameters.\\n          You can also answer questions about the dataset metadata, and explain statistical analyses.\"},\n",
       " {'role': 'user',\n",
       "  'content': 'Filter the dataset to include only 4 cylinder cars'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kwargs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mkwargs\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'kwargs' is not defined"
     ]
    }
   ],
   "source": [
    "kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
